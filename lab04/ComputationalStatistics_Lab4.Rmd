---
title: "Computational Statistics - Lab 04"
author: "Annalena Erhard (anner218) and Maximilian Pfundstein (maxpf364)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: false
    number_sections: false
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r libraries, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, include = TRUE, eval = TRUE)
library(knitr)
library(readxl)
library(ggplot2)
library(gridExtra)
library(coda)
set.seed(12345)
```

# Question 1: Computations with Metropolis-Hastings

Consider the following probability density function:

$$f(x) \propto x^5e^{-x}, \quad x > 0.$$

You can see that the distribution is known up to some constant of proportionality. If you are
interested (**NOT** part of the Lab) this constant can be found by applying integration by parts
multiple times and equals 120.

1. Use Metropolis–Hastings algorithm to generate samples from this distribution by using proposal distribution as log–normal $LN(X_t,1)$, take some starting point. Plot the chain you obtained as a time series plot. What can you guess about the convergence of the chain? If there is a burn–in period, what can be the size of this period?

**Answer:** First we will define the original function with the constant set to get a brief overview how the function actually looks like. We also define the given function without the constant, as we're going to us it later as the target distribution. We use the function without the constant to show that Metropilis-Hastings works without knowing the constant. As random samplings can actually give an $x$ outside of the scope of the function, we will set all values of $x \leq 0$ to a relatively small value.

```{r}

# Target function with original scaling
f = function(x) {
  return(120 * x^5 * exp(-x))
}

# Target function
df = function(x) {
  if (sum(c(x) <= 0) > 0) x = 0.0001
  return(x^5 * exp(-x))
}

```

```{r, echo = FALSE}

sequence = seq(from = 0.01, to = 20, by = 0.01)

real_f = f(sequence)
plotdf = data.frame(sequence, real_f)

ggplot(plotdf) +
  geom_line(aes(x = sequence, y = real_f), color = "#6091ec") +
  labs(title = "Target Density Function", y = "Density",
  x = "X", color = "Legend") +
  theme_minimal()

```

The following code chunk defines the Metropolis-Hastings algorithm. The correction factor `c` is used to handle the asymmetry in the proposal function. The code is commented.

```{r}

#' Metropolis Hasting Algorithm
#'
#' @param n Number of samples from the target distribution.
#' @param x_0 Initial state from Pi. 
#' @param b Burn-in steps to remove from samples.
#' @param proposal Selects the proposal function.
#' @param keep_burnin Decides if to keep the samples during the burn-in period.
#'
#' @return Returns a list containing samples.
#' @export
#'
#' @examples
metropolis_hastings = function(n = 1, x_0 = 1, b = 50, proposal = "lnorm",
                              keep_burnin = FALSE) {
  
  # Vectors to store the samples in
  samples = c()
  
  # Samples from proposel from the random walk (MC)
  xt = x_0
  xt_1 = x_0
  
  while (length(samples) < n) {
    
    # Generate proposal state
    if (proposal == "lnorm") {
      x_star = rlnorm(n = 1, meanlog = log(xt), sdlog = 1)
      
      # Calculate correction factor C
      c = dlnorm(xt_1, meanlog = xt, sdlog = 1) /
      dlnorm(x_star, meanlog = xt, sdlog = 1)
    }
    else if (proposal == "chisquared") {
      x_star = rchisq(n = 1, df = floor(xt))
      
       # Calculate correction factor C
      c = dchisq(x = xt_1, df = floor(xt)) /
      dchisq(x_star, df = floor(xt))
    }
    else {
      stop("Invalid proposal.")
    }
    
    # Calculate acceptance probability alpha
    if (df(xt_1) <= 0) {
      alpha = 0
    }
    else {
      alpha = min(1, df(x_star)/df(xt_1) * c)
    }
    
    # Generate u from uniform
    u = runif(n = 1, min = 0, max = 1)
    
    # Decide if to accept or reject the proposal
    if (u <= alpha) {
      # Accept
      xt_1 = xt
      xt = x_star
      samples = c(samples, x_star)
    }
    else {
      # Reject
      xt = xt_1
    }
  }
  
  # Return samples
  if (keep_burnin) return(samples)
  return(samples[b+1:length(samples)])
}

```

For the lognormal distribution as the proposal we selected a starting point which is far from the converging value to illistrate the burn-in period. We can see that we reach convergence rather quickly after around `8` iterations. We increased the limit to `10` to be sure to get into the area of the converging value.

```{r, echo = FALSE}

burnin = 10

results = metropolis_hastings(100, x_0 = 40, b = burnin, proposal = "lnorm",
                             keep_burnin = TRUE)

plotdf = data.frame(index = 1:length(results), values = results)

ggplot(plotdf) +
  geom_line(aes(x = index, y = values), color = "#6091ec") +
  labs(title = "Traceplot For The Burn-In Period (burnin = 10)", y = "X*",
  x = "Iteration", color = "Legend") +
  geom_vline(xintercept = burnin, color = "#C70039") + 
  theme_minimal()

```

The following plot shows `10.000` samples with $x_0 = 5$ as a starting point and burn-in iterations set to `100`.

```{r, echo = FALSE, warning = FALSE}

results_lnorm = metropolis_hastings(10000, x_0 = 5, b = 100, proposal = "lnorm")

plotdf = data.frame(results_lnorm)

ggplot(plotdf) +
  geom_histogram(aes(x = results_lnorm),
                 color = "#000000", fill = "#C70039", bins = length(results_lnorm)/100) +
  labs(title = "Samples From Target Function Using Normal Proposal", y = "Density",
  x = "X", color = "Legend") +
  theme_minimal()

```

2. Perform Step 1 by using the chi–square distribution $\chi ^2(\lfloor X_t+1 \rfloor)$ as a proposal distribution, where $\lfloor x \rfloor$ is the floor function, meaning the integer part of $x$ for positive $x$, i.e. $\lfloor 2.95 \rfloor = 2.$

**Answer:** The trace plot shows the burn-in period for the chi-square distribution. We see that it takes quite longer to achieve convergence. We set the burn-in iterations to `75` to be sure to converge.

```{r, echo = FALSE}

burnin = 75

results = metropolis_hastings(300, x_0 = 500, b = burnin, proposal = "chisquared", keep_burnin = TRUE)

plotdf = data.frame(index = 1:length(results), values = results)

ggplot(plotdf) +
  geom_line(aes(x = index, y = values), color = "#6091ec") +
  labs(title = "Traceplot For The Burn-In Period (burnin = 75)", y = "X*",
  x = "Iteration", color = "Legend") +
  geom_vline(xintercept = burnin, color = "#C70039") + 
  theme_minimal()

```

Again you can see  `10.000` samples with $x_0 = 5$ as a starting point and burn-in iterations set to `75`.

```{r, echo = FALSE, warning = FALSE}

results_chisquared = metropolis_hastings(10000, x_0 = 5, b = 100, proposal = "chisquared")

plotdf = data.frame(results_chisquared)

ggplot(plotdf) +
  geom_histogram(aes(x = results_chisquared),
                 color = "#000000", fill = "#C70039", bins = length(results_chisquared)/100) +
  labs(title = "Samples From Target Function Using Chi-Squared Proposal", y = "Density",
  x = "X", color = "Legend") +
  theme_minimal()

```

3. Compare the results of Steps 1 and 2 and make conclusions.

**Answer:** The samples seem to actually be taken from the real distribution and in this context ist doesn't matter which proposal distribution we take. Comparing the burn-in times we see that it seems to take longer for the chi-squared proposal to converge. We have to keep in mind that this mostly depends on the initial state of the MCMC. It's not so easy to take an initial value which is far of from the converging value as we get values around `0` as the functions have a really low density which makes it quite hard to actually calcualte `c` and $\alpha$. As long as we take an initial state not too far off from a low density, we shoudl be fine, as the algorithm automatically avoids really low density regions.

We assume as long as the proposal distribution mostly covers the target distribution and the initial state is selected quite reqsonable (which is not always the case in the "real world") the Metropolis-Hastings works quite well.

4. Generate 10 MCMC sequences using the generator from Step 2 and starting points 1, 2, ..., or 10. Use the Gelman–Rubin method to analyze convergence of these sequences.

**Answer:** The following output is based three sets of multiple sequences (with starting points $1...10$). The first set of sequences uses `5`, the second `10` and the last uses `50` iterations. We can see that the Upper C.I. (at `95%` confidence) converges towards `1` which indicates that the set has converged. This works by comparing the variance of the sequences within a set. As more iterations are performed, the MCMC depends less on the starting point until it finally holds no more information about that. This is when the Gelman-Ruby factor is near `1`.

```{r, echo = FALSE}

results5 = list()
results10 = list()
results50 = list()
k = 10

for (i in 1:k) {
  results5[[i]] = mcmc(metropolis_hastings(5, x_0 = i, b = 0,
                                   proposal = "chisquared", keep_burnin = TRUE))
}

for (i in 1:k) {
  results10[[i]] = mcmc(metropolis_hastings(10, x_0 = i, b = 0,
                                   proposal = "chisquared", keep_burnin = TRUE))
}

for (i in 1:k) {
  results50[[i]] = mcmc(metropolis_hastings(50, x_0 = i, b = 0,
                                   proposal = "chisquared", keep_burnin = TRUE))
}

mcmc_list5 = mcmc.list(results5)
gelman.diag(mcmc_list5)

mcmc_list10 = mcmc.list(results10)
gelman.diag(mcmc_list10)

mcmc_list50 = mcmc.list(results50)
gelman.diag(mcmc_list50)

```

5. Estimate

$$\int_{0}^{\infty} x f(x)dx $$
using the samples from Steps 1 and 2.

```{r}

f_integrate = function(x) {
  return(x * df(x))
}

valid_samples_lnorm = results_lnorm[!is.na(results_lnorm)]

b = max(valid_samples_lnorm)
a = min(valid_samples_lnorm)
diff = b - a

sum(f_integrate(valid_samples_lnorm) * diff / df(valid_samples_lnorm)) /
  length(valid_samples_lnorm)


```

```{r}

valid_samples_chisquared = results_chisquared[!is.na(results_chisquared)]

b = max(valid_samples_chisquared)
a = min(valid_samples_chisquared)
diff = b - a

sum(f_integrate(valid_samples_chisquared) * diff / df(valid_samples_chisquared)) /
  length(valid_samples_chisquared)

```


6. The distribution generated is in fact a gamma distribution. Look in the literature and define the actual value of the integral. Compare it with the one you obtained.

# Question 2: Gibbs Sampling

concentration of a certain chemical was measured in a water sample, and the result was stored
in the data `chemical.RData` having the following variables:

- $X:$ day of the measurement
- $Y:$ measured concentration of the chemical.

The instrument used to measure the concentration had certain accuracy; this is why the measurements can be treated as noisy. Your purpose is to restore the expected concentration values.

1. Import the data to R and plot the dependence of Y on X. What kind of model is reasonable to use here?

2. A researcher has decided to use the following (random–walk) Bayesian model (n=numberof observations, $\vec{\mu} = (\mu_1,...,\mu_n)$ are unknown parameters):

$$Y_i = \mathcal{N}(\mu_i, \text{variance} = 0.2), \quad i=1,...,n$$
where the prior is

$$p(\mu_1) = 1$$

$$p(\mu_{i+1}|\mu_i) = \mathcal{N}(\mu_i, 0.2), i=1,...,n1.$$

Present the formulae showing the likelihood $p(\vec{Y}|\vec{\mu})$ and the prior $p(\vec{\mu})$. **Hint:** a chain rule can be used here $p(\vec{\mu}) = p(\mu_1)p(\mu_2|\mu_1)p(\mu_3|\mu_2)...p(\mu_n|\mu_n1)$

3. Use Bayes’ Theorem to get the posterior up to a constant proportionality, and then find out the distributions of $(\mu_i|\vec{\mu}_{-i},\vec{Y})$, where $\mu_{-i}$ is a vector containing all $\mu$ values except of $\mu_{i}$.

Hint A: consider for separate formulae for $(\mu_1|\vec{\mu}_{-1},\vec{Y}), (\mu_n|\vec{\mu}_{-n},\vec{Y})$ and then a formula for all remaining $(\mu_i|\vec{\mu}_{-i},\vec{Y})$.

Hint B:

$$e^{-\frac{1}{d}\left( (x-a)^2 + (x-b)^2 \right)} \propto e^{-\frac{(x-(a+b)/2)^2}{d/2}}$$

Hint C:

$$e^{-\frac{1}{d}\left( (x-a)^2 + (x-b)^2 + (x-c)^2 \right)} \propto e^{-\frac{(x-(a+b+c)/3)^2}{d/3}}$$

4. Use the distributions derived in Step 3 to implement a Gibbs sampler that uses $\vec{\mu}^{\,0} = 0,...,0$ as a starting point. Run the Gibbs sampler to obtain 1000 values of $\vec{\mu}$ and then compute the expected value of $\vec{\mu}$ versus X and Y versus X in the same graph. Does it seem that you have managed to remove the noise? Does it seem that the expected value of $\vec{\mu}$ can catch the true underlying dependence between Y and X?

5. Make a trace plot for $\mu_n$ and comment on the burn–in period and convergence.



```{r}

# Loading RData
#data2 = get(load("data.RData"))
#head(data2)

```



# Source Code

```{r, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE, results = 'show'}

```